# 如何训练一个神经网络 第一部分

## 激活函数
### 激活函数是干什么用的?
激活函数是用来加入非线性特性的函数，它们通常被用在神经网络的隐藏层和输出层，以帮助网络学习复杂的模式和关系。常见的激活函数包括Sigmoid函数、ReLU函数和tanh函数等

###  为什么激活函数是用来加入非线性特性的?
激活函数被用来加入非线性特性，是因为线性函数的叠加仍然是线性的。在神经网络中，如果没有非线性的激活函数，多层神经网络就会退化成单层网络，无法学习复杂的非线性模式;

通过使用非线性的激活函数，神经网络可以学习和表示更加复杂的函数关系，因为非线性函数可以引入曲线和弯曲，从而使神经网络能够逼近任意复杂的函数。这种非线性特性使得神经网络成为强大的模式识别和函数逼近工具，能够处理现实世界中的复杂数据和问题

## Data Preprocessing

### Zero-centered
数据集中的每个特征都减去该特征的均值, 使得每个特征的均值为0, 这种预处理方法可以使得数据集在每个特征中都已0为中心

可以消除特征之间的偏移, 使得数据更容易被处理和分析; zero-centered也有助于加快收敛速度, 提高模型的训练效率

### 归一化
将数据按比例缩放, 使其落入一个特定的范围, 通常是[0, 0]或者[-1, 1], 归一化使得不同特征具有可比性, 有利于提高模型收敛的速度, 避免某些特征对模型训练影响过大

### 批量归一化
通过在神经网络的每一层的输入上进行归一化, 使得每层的输入都具有零均值和单位方差, 这有助于解决梯度消失和梯度爆炸, 加速模型的收敛速度

批量归一化通常被应用在神经网络的全连接层或卷积层之后, 减少过拟合